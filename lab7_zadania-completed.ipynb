{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "262bd739-4516-4bee-9ade-63c6d709c795",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab 7 — Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow\n",
    "\n",
    "## 🔄 Wprowadzenie\n",
    "\n",
    "W tym laboratorium zapoznasz sie z roznymi metodami zasilania danych strumieniowych w Apache Spark oraz zastosowaniem prostych transformacji, filtrowania i segmentacji klientow w czasie rzeczywistym.\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Pomocnicza funkcja do wyswietlania naszych danych strumieniowych\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6432d01f-a480-4b20-a7be-88a2fa4c17f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0e684e4-f273-4c69-8959-f8d4f1ac433e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 🔹 rate jako źródło kontrolowanego strumienia\n",
    "\n",
    "### ✅ Zadanie 1\n",
    "\n",
    "1. Przygotuj strumien danych z `format('rate')`, ustaw `rowsPerSecond` na 5.\n",
    "2. Utworz kolumne `user_id`: `expr(\"concat('u', cast(rand()*100 as int))\")`\n",
    "3. Dodaj kolumne `event_type`: `expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d286d0c-ceac-4f57-b7a4-dade856bac7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "rate_df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 5)\n",
    "      .load())\n",
    "\n",
    "events = (rate_df.withColumn(\"user_id\", expr(\"concat('u', cast(rand()*100 as int))\"))\n",
    "          .withColumn(\"event_type\", expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\")))\n",
    "    \n",
    "query = (events.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29adcce4-3cf4-42ed-b161-70cf152116a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 🔹 Filtrowanie danych bez agregacji (append mode)\n",
    "\n",
    "Zobacz jak działa poniższy kod i na jego podstawie:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AppendExample\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Źródło rate - generuje timestamp + value\n",
    "rate_df = (spark.readStream\n",
    "           .format(\"rate\")\n",
    "           .option(\"rowsPerSecond\", 5)\n",
    "           .load())\n",
    "\n",
    "# Filtracja bez potrzeby agregacji (bezstanowe przetwarzanie)\n",
    "filtered = rate_df.filter(col(\"value\") % 2 == 0) \\\n",
    "                  .withColumn(\"info\", expr(\"concat('even:', value)\"))\n",
    "\n",
    "# outputMode = append → pokazuje tylko nowe wiersze, bez stanu\n",
    "query = (filtered.writeStream \n",
    "    .outputMode(\"append\") \n",
    "    .format(\"console\") \n",
    "    .option(\"truncate\", False) \n",
    "    .foreachBatch(process_batch)\n",
    "    .start()\n",
    "        )\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "1. Skorzystaj z danych z poprzedniego zadania.\n",
    "2. Wyfiltruj tylko `purchase`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c60411-cc9f-48ff-9f42-42a48203f425",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col\n",
    " \n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    " \n",
    "rate_df = (spark.readStream\n",
    "      .format(\"rate\")\n",
    "      .option(\"rowsPerSecond\", 5)\n",
    "      .load()\n",
    ")\n",
    " \n",
    "events = (rate_df.withColumn(\"user_id\", expr(\"concat('u', cast(rand()*100 as int))\"))\n",
    "          .withColumn(\"event_type\", expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\"))\n",
    "          .select(\"timestamp\", \"user_id\", \"event_type\")\n",
    "         )\n",
    "\n",
    "\n",
    "purchases = events.filter(col(\"event_type\") == \"purchase\")\n",
    "\n",
    "query = (purchases.writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"append\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b54ea43-0fad-4813-8e24-e97f691be8f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 🔹 Źródło plikowe (JSON)\n",
    "\n",
    "### ✅ Generator danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3b28f8-fd2a-4c34-84a7-9a5669c502e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: True"
     ]
    }
   ],
   "source": [
    "# Odpalałem na databricks community edition, dlatego trzeba kombinować w ten sposób\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/tables/data/stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d347ac8d-fe3e-4525-bca4-b1be43bb3eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generator.py\n"
     ]
    }
   ],
   "source": [
    "%%file generator.py\n",
    "# generator.py\n",
    "import json, os, random, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "output_dir = \"/dbfs/FileStore/tables/data/stream\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "event_types = [\"view\", \"cart\", \"purchase\"]\n",
    "categories = [\"electronics\", \"books\", \"fashion\", \"home\", \"sports\"]\n",
    "\n",
    "def generate_event():\n",
    "    return {\n",
    "        \"user_id\": f\"u{random.randint(1, 50)}\",\n",
    "        \"event_type\": random.choices(event_types, weights=[0.6, 0.25, 0.15])[0],\n",
    "        \"timestamp\": (datetime.utcnow() - timedelta(seconds=random.randint(0, 300))).isoformat(),\n",
    "        \"product_id\": f\"p{random.randint(100, 120)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"price\": round(random.uniform(10, 1000), 2)\n",
    "    }\n",
    "\n",
    "# Simulate file-based streaming\n",
    "while True:\n",
    "    batch = [generate_event() for _ in range(50)]\n",
    "    filename = f\"{output_dir}/events_{int(time.time())}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for e in batch:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "    print(f\"Wrote: {filename}\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84ad5185-2867-4260-a5ad-065f731cd1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ Schemat danych:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_id\": \"u123\",\n",
    "  \"event_type\": \"purchase\", // albo \"view\", \"cart\", \"click\"\n",
    "  \"timestamp\": \"2025-05-09T15:24:00Z\",\n",
    "  \"product_id\": \"p456\",\n",
    "  \"category\": \"electronics\",\n",
    "  \"price\": 299.99\n",
    "}\n",
    "```\n",
    "\n",
    "1. Utwórz zmienną `schema`, która zrealizuje schamat danych naszej ramki. Wykorzystaj `StringType()`, `TimestampType()`,`DoubleType()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b73faeea-18a1-4509-971b-a3439fcf81ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RealTimeEcommerce\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# StringType(), TimestampType(), DoubleType()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8bbcca6-e295-4e07-b3b0-2772cd6f06a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ✅ Odczyt danych z katalogu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74f1f1e-4e16-4866-90e9-c3d606c78f37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"dbfs:/FileStore/tables/data/stream\"))\n",
    "\n",
    "query = (stream.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "decca853-5c98-4abd-9718-da9ebe498514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 🔹 Bezstanowe zliczanie zdarzen\n",
    "\n",
    "1. Przygotuj zmienną agg1 zliczającą zdarzenia należące do danej grupy `event_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f71fa35-3b08-4eb0-8ad7-539127d63fad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "agg1 = (stream.groupBy(\"event_type\").count())\n",
    "\n",
    "# pamietaj, że agregacje wymagają opcji complete\n",
    "query = (agg1\n",
    "         .writeStream\n",
    "         .outputMode(\"complete\")\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e892e020-7f08-4454-b79c-4f2a6240dbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 🔹 Agregacja w oknach czasowych\n",
    "\n",
    "`withWatermark(\"timestamp\", \"1 minute\")`\n",
    "   \n",
    "💡 Do czego służy: Informuje Sparka, że dane przychodzą z opóźnieniem i należy je przetwarzać tylko do określonego limitu wstecz (tutaj: 1 minuta).\n",
    "\n",
    "🚨 Dlaczego ważne: Bez watermarku Spark trzymałby w pamięci wszystkie dane, by móc je jeszcze pogrupować. Watermark pozwala zwolnić pamięć.\n",
    "\n",
    "1. Pogrupuj typy zdarzen w thumbling window, w oknie co 5 minut\n",
    "2. dodaj watermark z ustawieniem na 1 minutę. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4d7e264-4504-48ab-9b1f-ef9aac3a1445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowed = (stream.withWatermark(\"timestamp\", \"1 minute\")\n",
    "            .groupBy(window(\"timestamp\", \"5 minutes\"), \"event_type\").count())\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0aefac8-51db-4f94-8abb-4b679462ec81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Zmień thumbling window na sliding window z szerokością okna 5 minut i startem nowego okna co 1 minutę. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "373d18c4-cb6a-42ca-9bc2-7361e3964ad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "windowed = (stream.withWatermark(\"timestamp\", \"1 minute\")\n",
    "            .groupBy(window(\"timestamp\", \"5 minutes\", \"1 minute\"), \"event_type\").count())\n",
    "\n",
    "query = (\n",
    "    windowed.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .foreachBatch(process_batch)\n",
    "    .format(\"console\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6299f1-7d3d-41cc-8704-7d963a1638b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 🔹 Segmentacja klientow\n",
    "\n",
    "🧩 Logika segmentacji:\n",
    "\n",
    "1. jeśli był purchase → \"Buyer\"\n",
    "2. jeśli był cart, ale nie purchase → \"Cart abandoner\"\n",
    "3. jeśli tylko view → \"Lurker\"\n",
    "\n",
    "\n",
    "`groupBy(window(...), \"user_id\")`\n",
    "\n",
    "💡 Do czego służy: Grupujemy dane per użytkownik w konkretnym przedziale czasu (oknie 5-minutowym).\n",
    "\n",
    "⏱️ window(\"timestamp\", \"5 minutes\"): Funkcja okna czasowego – każda grupa będzie dotyczyć jednego użytkownika w konkretnym 5-minutowym interwale.\n",
    "\n",
    "`agg(collect_set(\"event_type\"))`\n",
    "\n",
    "💡 Do czego służy: Zbiera wszystkie typy zdarzeń (view, cart, purchase) danego użytkownika w danym oknie.\n",
    "\n",
    "🧠 Dlaczego `collect_set` a nie `collect_list`?: collect_set usuwa duplikaty — interesuje nas tylko czy coś się zdarzyło, a nie ile razy.\n",
    "\n",
    "5. withColumn(... expr(...))\n",
    "\n",
    "💡 Do czego służy: Na podstawie zbioru zdarzeń określamy segment użytkownika.\n",
    "\n",
    "🔎 `array_contains`: Funkcja sprawdzająca, czy dany typ zdarzenia znajduje się w tablicy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "🧠 Co warto wiedzieć:\n",
    "\n",
    "- Segmentacja to klasyczne zastosowanie agregacji i transformacji strumienia.\n",
    "- Łączenie window + watermark jest kluczowe do kontroli stanu.\n",
    "- collect_set umożliwia prostą analizę zachowań, bez potrzeby przechowywania surowych danych.\n",
    "- expr() daje elastyczność, by używać składni SQL wewnątrz kodu DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995639b5-27d6-42d7-8b8c-a2b5df1ce166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "segmented = (\n",
    "    stream\n",
    "      .withWatermark(\"timestamp\", \"1 minute\")\n",
    "      .groupBy(window(\"timestamp\", \"5 minutes\"), \"user_id\")\n",
    "      .agg(collect_set(\"event_type\").alias(\"events\"))\n",
    "      .withColumn(\"segment\",\n",
    "          when(array_contains(col(\"events\"), \"purchase\"), \"Buyer\") # Moim zdaniem lepsza składnia niż użycie expr()\n",
    "         .when(array_contains(col(\"events\"), \"cart\"), \"Cart abandoner\")\n",
    "         .otherwise(\"Lurker\")\n",
    "      )\n",
    "      .select(\"window\", \"user_id\", \"segment\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    segmented.writeStream\n",
    "      .outputMode(\"update\")\n",
    "      .format(\"console\")\n",
    "      .option(\"truncate\", False)\n",
    "      .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51b5f16-4875-4d0b-9621-9ea3ea5d7a98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+-------+--------------+\n|window                                    |user_id|segment       |\n+------------------------------------------+-------+--------------+\n|{2025-05-09 00:00:00, 2025-05-09 00:05:00}|u1     |Buyer         |\n|{2025-05-09 00:00:00, 2025-05-09 00:05:00}|u2     |Cart abandoner|\n|{2025-05-09 00:00:00, 2025-05-09 00:05:00}|u3     |Lurker        |\n|{2025-05-09 00:05:00, 2025-05-09 00:10:00}|u4     |Buyer         |\n+------------------------------------------+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, collect_set, when, array_contains, col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "# Z uwagi na ograniczenia w databricks community edition, na którym to odpalam, dla pokazania działania segmentacji wyprintuje na \"twardo\" segmentacje - w zasadzie takie testy jednostkowe\n",
    "data = [\n",
    "    # Buyer\n",
    "    (\"u1\", \"view\", \"2025-05-09T00:01:00Z\", \"p101\", \"electronics\", 299.99),\n",
    "    (\"u1\", \"cart\", \"2025-05-09T00:02:00Z\", \"p101\", \"electronics\", 299.99),\n",
    "    (\"u1\", \"purchase\", \"2025-05-09T00:03:00Z\", \"p101\", \"electronics\", 299.99),\n",
    "    \n",
    "    # Cart abandoner\n",
    "    (\"u2\", \"view\", \"2025-05-09T00:01:00Z\", \"p102\", \"books\", 19.99),\n",
    "    (\"u2\", \"cart\", \"2025-05-09T00:04:00Z\", \"p102\", \"books\", 19.99),\n",
    "    \n",
    "    # Lurker\n",
    "    (\"u3\", \"view\", \"2025-05-09T00:02:00Z\", \"p103\", \"fashion\", 49.99),\n",
    "    \n",
    "    # Buyer, ale inny time window\n",
    "    (\"u4\", \"view\", \"2025-05-09T00:07:00Z\", \"p104\", \"home\", 129.99),\n",
    "    (\"u4\", \"purchase\", \"2025-05-09T00:08:00Z\", \"p104\", \"home\", 129.99)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(\"timestamp\"))\n",
    "\n",
    "segmented = (\n",
    "    df\n",
    "      .groupBy(window(\"timestamp\", \"5 minutes\"), \"user_id\")\n",
    "      .agg(collect_set(\"event_type\").alias(\"events\"))\n",
    "      .withColumn(\"segment\",\n",
    "          when(array_contains(col(\"events\"), \"purchase\"), \"Buyer\")\n",
    "         .when(array_contains(col(\"events\"), \"cart\"), \"Cart abandoner\")\n",
    "         .otherwise(\"Lurker\")\n",
    "      )\n",
    "      .select(\"window\", \"user_id\", \"segment\")\n",
    ")\n",
    "\n",
    "segmented.orderBy(\"window.start\", \"user_id\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc92ffcb-9d81-494b-99e2-f9a359f60837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## ⚙️ Porownanie trybow `outputMode`\n",
    "\n",
    "| outputMode | Opis                                        | Kiedy uzywac                           | Wymagania                 |\n",
    "| ---------- | ------------------------------------------- | -------------------------------------- | ------------------------- |\n",
    "| `append`   | Wypisywane sa **tylko nowe wiersze**        | Filtrowanie, wzbogacanie bez agregacji | Nie dziala z `groupBy`    |\n",
    "| `update`   | Wypisywane sa tylko **zmienione wiersze**   | Agregacje z watermarkami               | Wymaga zarzadzania stanem |\n",
    "| `complete` | Wypisywane jest **calosciowe podsumowanie** | Podsumowania okien, snapshoty          | Moze byc kosztowne        |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lab7_zadania",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}